---
title: "Notes on Anthropic's Prompt Engineering Interactive Tutorial"
---

# Anthropic's Prompt Engineering Interactive Tutorial

## core parameters

- `max_tokens` is the maximum number of tokens that the model can generate. It is a hard stop. It may stop an output mid-word or mid-sentence.
- `messages` are an array of alternating user and assistant messages. The assistant messages are the ones generated by the model.
    - each message must have a `role` and `content` 
    - roles can be `user` or `assistant`
    - they must alternate between user and assistant and start with a user turn. you can use both to simulate a conversation.

## optional parameters

- `temperature` is a hyperparameter that controls randomness in the model's output. It is a value between 0 and 1. Lower values make the model more deterministic and higher values make it more random.
- `system` is the system prompt that the model uses to generate the response. It provides instructions or guidelines.

## golden rule of clear prompting

- "Show your prompt to a colleague or friend and have them follow the instructions themselves to see if they can produce the result you want. If they're confused, Claude's confused."

## role prompting

- As explained in the name, you can prompt an LLM to take on a specific role in a conversation.
- This can happen in the `system` prompt, in `messages` array in the user turn.

## prompt templates

- `prompt templates` are a way to structure your prompts to include
- done by separating the fixed skeleton of the prompt from variable user input, then substituting the user input into the prompt skeleton
- you can wrap the input in xml tags to make it easier to identify and replace. `<input` and `</input>` are the tags used. they are a useful prompt organizing pattern.

```{python}
EMAIL = "Show up at 6am tomorrow because I'm the CEO and I say so."

PROMPT = f"Yo Claude. <email>{EMAIL}</email> <----- Make this email more polite but don't change anything else about it."
```

## prefill responses

- You can prefill the assistant message with a partial response to guide the model in the right direction.
- This could be used to direct the LLM toward an XML tag or JSON based response.

## `stop_sequences` parameter

- The `stop_sequences` parameter in the Anthropic API lets you save money by stopping the model from generating more tokens once it reaches a certain point.
- For example, you could pass the closing XML tag through the `stop_sequences` parameter to stop the model from generating more tokens after the closing tag is generated.

## step-by-step reasoning

- You can ask the model to explain its reasoning step-by-step before generating the final output. 

## n-shot prompting

- `n-shot prompting` is a technique where you provide the model with multiple examples of the same task to help it answer better.

## Avoiding hallucinations

- Give the LLM an out by telling it that it's okay to decline to answer or only answer with certainty: "Only answer if you know the answer with certainty"
- Make the LLM gather evidence first. You can do this by asking it to first extract relevant quotes in a <SCRATCHPAD> then base a response on that evidence.
    - Example: "Please read the below document. Then, in <scratchpad> tags, pull the most relevant quote from the document and consider whether it answers the user's question or whether it lacks sufficient detail. Then write a brief numerical answer in <answer> tags."
    
## `temperature` parameter

- Temperature is a measurement of answer creativity between 0 and 1, with 1 being more unpredictable and less standardized, and 0 being the most consistent. 
- Asking Claude something at temperature 0 will generally yield an almost-deterministic answer set across repeated trials (although complete determinism is not guaranteed). Asking Claude something at temperature 1 (or gradations in between) will yield more variable answers.

## Prompt chaining

- When you pass the result of one prompt as the input to another prompt, you are chaining prompts.
- You can do this via the messages list by adding an element to the messages list that has the role of the assistant and the content of the previous prompt's output.
- This is a useful way to make the responses from an LLM more coherent and context-aware.

## Tool use 

Follow this process:
1. Output the tool name and arguments it wants to call
2. Halt any further response generation while the tool is called
3. Then we reprompt with the appended tool results

## Setting up tool use

- Done using the system prompt where you provide a description of the tool and the arguments it expects, output, and how to call it. Same applies to the list of tools.
- The goal is to get the LLM to decide which tool it will need and output tool commands. You can then uses these commands in your computing environment to actually call the tool. The you return it back to the LLM usually in a prompt chained onto the conversation. 
- The useful thing about tool use is that it formats the output in a way that you can then use in your computing environment to actually call a real tool.

## Additional learning resources on tool use

- [Connecting LLMs to tools - Google Developers](https://www.youtube.com/watch?v=P3buv6P_u7c&ab_channel=GoogleforDevelopers)
- [Building LLM Agents with Tool Use - Jay Alammar](https://www.youtube.com/watch?v=5drn2DO7gNY&ab_channel=JayAlammar) - great showcase of LangChain tool library in jupyter notebook
- [LLM Function Calling - AI Tools Deep Dive](https://www.youtube.com/watch?v=gMeTK6zzaO4&t=351s&ab_channel=AdamLucek) - good exploration in jupyter notebook of tool calls